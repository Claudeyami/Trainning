"""
G1 Standing Environment - VERSION ƒê∆†N GI·∫¢N
Ch·ªâ ƒë·ªÉ robot ƒë·ª©ng th·∫≥ng, kh√¥ng nh·∫£y l√™n cao
"""

import numpy as np
import gymnasium as gym
from gymnasium import spaces
import mujoco
import yaml
import os
import math
from typing import Dict, Any, Tuple, Optional


class G1StandingSimple(gym.Env):
    """
    G1 Standing Environment - ƒê∆†N GI·∫¢N
    Ch·ªâ ƒë·ªÉ robot ƒë·ª©ng th·∫≥ng ·ªü ƒë·ªô cao th·∫•p
    """
    
    def __init__(
        self,
        xml_path: str = "g1_description/g1_23dof.xml",
        config_path: str = "configs/g1_train_stand_fixed.yaml",
        render_mode: Optional[str] = None,
        max_episode_steps: int = 1000,
        terminate_on_fall: bool = True,
    ):
        super().__init__()
        
        # Load config
        self.xml_path = xml_path
        self.config_path = config_path
        self.render_mode = render_mode
        self.max_episode_steps = max_episode_steps
        self.terminate_on_fall = terminate_on_fall
        self.current_step = 0
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # MuJoCo setup
        self.model = mujoco.MjModel.from_xml_path(xml_path)
        self.data = mujoco.MjData(self.model)
        self.viewer = None
        
        # Thi·∫øt l·∫≠p tr·ªçng l·ª±c v√† physics
        self.model.opt.gravity[2] = -9.81
        self.model.opt.timestep = 0.002
        self.model.opt.integrator = 0  # Euler
        
        # Control parameters
        self.dt = 0.002
        self.decim = 5
        self.action_scale = 0.05  # V·ª™A PH·∫¢I ƒë·ªÉ ƒë·ª©ng t·ª± nhi√™n
        
        # PD Control gains - M·∫†NH H·ªöN ƒë·ªÉ gi·ªØ thƒÉng b·∫±ng
        self.default = np.array([-0.12, 0.00, +0.08, +0.34, -0.16, +0.02, -0.12, 0.00, +0.08, +0.34, -0.16, +0.02], dtype=np.float32)
        self.kp = np.array([150, 120, 150, 240, 150, 75, 150, 120, 150, 240, 150, 75], dtype=np.float32)
        self.kd = np.array([8, 8, 8, 12, 8, 5, 8, 8, 8, 12, 8, 5], dtype=np.float32)
        
        # Action v√† observation space
        self.num_actions = 12
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.num_actions,), dtype=np.float32)
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(47,), dtype=np.float32)
        
        # Actuator limits
        self.ctrl_min = None
        self.ctrl_max = None
        
        # Base body ID
        try:
            self.base_bid = self.model.body("pelvis").id
        except:
            self.base_bid = 0
        
        # L·∫•y 12 joints ƒë·∫ßu ti√™n
        self.actuated_joint_ids = []
        for i in range(min(self.model.nu, self.num_actions)):
            trn_type = self.model.actuator_trntype[i]
            if trn_type == mujoco.mjtTrn.mjTRN_JOINT:
                j_id = int(self.model.actuator_trnid[i, 0])
                self.actuated_joint_ids.append(j_id)
        
        # State tracking
        self.prev_action = np.zeros(self.num_actions, dtype=np.float32)
        
        # Assist system - H·ªñ TR·ª¢ ƒê·ªÇ ƒê·ª®NG V·ªÆNG
        self.assist_enabled = True
        self.assist_strength = 0.8  # ƒê·ªô m·∫°nh c·ªßa assist - TƒÇNG M·∫†NH
        self.assist_decay = 0.995  # Gi·∫£m d·∫ßn theo th·ªùi gian - CH·∫¨M H·ªöN
        
        # Governor system - KI·ªÇM SO√ÅT H√ÄNH ƒê·ªòNG
        self.governor_enabled = True
        self.max_base_velocity = 2.0  # T·ªëc ƒë·ªô t·ªëi ƒëa c·ªßa base
        self.max_pitch_angle = 0.3  # G√≥c nghi√™ng t·ªëi ƒëa
        
        print(f"üéØ G1StandingSimple - ƒê·ª®NG TH·∫≤NG!")
        print(f"  - Action scale: {self.action_scale}")
        print(f"  - PD gains: kp={self.kp[:3]}, kd={self.kd[:3]}")
    
    def _get_observation(self):
        """T·∫°o observation vector"""
        # L·∫•y joint positions v√† velocities
        q = []
        dq = []
        for joint_id in self.actuated_joint_ids:
            qpos_idx = self.model.jnt_qposadr[joint_id]
            qvel_idx = self.model.jnt_dofadr[joint_id]
            q.append(self.data.qpos[qpos_idx])
            dq.append(self.data.qvel[qvel_idx])
        
        q = np.array(q, dtype=np.float32)
        dq = np.array(dq, dtype=np.float32)
        omega = self.data.qvel[3:6].copy()
        quat = self.data.qpos[3:7].copy()
        
        # Phase for standing
        phase = 0.0
        ph = np.array([np.sin(phase), np.cos(phase)], dtype=np.float32)
        
        return np.concatenate([
            omega * 0.25,
            self._gravity_orientation(quat),
            np.array([0.0, 0.0, 0.0]),  # cmd
            (q - self.default) * 1.0,
            dq * 0.05,
            np.zeros(self.num_actions),  # prev_action
            ph
        ], axis=0).astype(np.float32)
    
    def _gravity_orientation(self, quat):
        """Convert quaternion to gravity vector"""
        qw, qx, qy, qz = quat
        g0 = 2 * (-qz * qx + qw * qy)
        g1 = -2 * (qz * qy + qw * qx)
        g2 = 1 - 2 * (qx*qx + qy*qy)
        return np.array([g0, g1, g2], dtype=np.float32)
    
    def _pd_control(self, q, dq, q_ref):
        """PD Controller"""
        tau = self.kp * (q_ref - q) + self.kd * (0.0 - dq)
        return tau
    
    def _compute_assist_torque(self, q, dq):
        """Assist system - H·ªó tr·ª£ ƒë·ªÉ ƒë·ª©ng v·ªØng"""
        assist_tau = np.zeros(self.num_actions, dtype=np.float32)
        
        if len(q) >= 12:
            # Assist cho ch√¢n ƒë·ªÉ gi·ªØ thƒÉng b·∫±ng - M·∫†NH H·ªöN
            for i in range(6):  # 6 joints ch√¢n tr√°i
                if i < len(q):
                    # Assist v·ªÅ v·ªã tr√≠ m·∫∑c ƒë·ªãnh
                    error = self.default[i] - q[i]
                    assist_tau[i] = 0.3 * error  # Assist M·∫†NH H·ªöN
            
            for i in range(6, 12):  # 6 joints ch√¢n ph·∫£i
                if i < len(q):
                    # Assist v·ªÅ v·ªã tr√≠ m·∫∑c ƒë·ªãnh
                    error = self.default[i] - q[i]
                    assist_tau[i] = 0.3 * error  # Assist M·∫†NH H·ªöN
            
            # Assist cho vi·ªác gi·ªØ thƒÉng b·∫±ng - TH√äM M·ªöI
            if len(q) >= 6:
                # Assist cho base ƒë·ªÉ gi·ªØ th·∫≥ng
                base_roll = q[3] if len(q) > 3 else 0
                base_pitch = q[4] if len(q) > 4 else 0
                assist_tau[3] += -0.5 * base_roll  # Gi·∫£m roll
                assist_tau[4] += -0.5 * base_pitch  # Gi·∫£m pitch
        
        return assist_tau
    
    def _apply_governor(self, tau, q, dq):
        """Governor system - Ki·ªÉm so√°t h√†nh ƒë·ªông"""
        # Gi·ªõi h·∫°n t·ªëc ƒë·ªô base
        if len(dq) >= 6:
            base_velocity = np.linalg.norm(dq[:3])  # T·ªëc ƒë·ªô linear
            if base_velocity > self.max_base_velocity:
                scale = self.max_base_velocity / base_velocity
                tau[:6] *= scale
        
        # Gi·ªõi h·∫°n g√≥c nghi√™ng
        if len(q) >= 6:
            pitch_angle = abs(q[4])  # G√≥c pitch
            if pitch_angle > self.max_pitch_angle:
                # Gi·∫£m torque n·∫øu nghi√™ng qu√° m·ª©c
                scale = self.max_pitch_angle / pitch_angle
                tau *= scale
        
        return tau
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None) -> Tuple[np.ndarray, Dict]:
        """Reset environment"""
        super().reset(seed=seed)
        
        self.current_step = 0
        mujoco.mj_resetData(self.model, self.data)
        
        # Set initial pose
        for i, joint_id in enumerate(self.actuated_joint_ids):
            qpos_idx = self.model.jnt_qposadr[joint_id]
            if i < len(self.default):
                self.data.qpos[qpos_idx] = self.default[i]
        
        # Set height - THEO CHI·ªÄU CAO TH·ª∞C T·∫æ C·ª¶A ROBOT (0.793m)
        self.data.qpos[2] = 0.793
        
        # Set upright orientation
        self.data.qpos[3:7] = [1.0, 0.0, 0.0, 0.0]
        
        # Add small noise
        noise_scale = 0.005
        qpos_noise = self.np_random.normal(0, noise_scale, size=self.data.qpos.shape)
        qvel_noise = self.np_random.normal(0, noise_scale * 0.1, size=self.data.qvel.shape)
        qpos_noise[3:7] = 0.0
        self.data.qpos += qpos_noise
        self.data.qvel += qvel_noise
        
        mujoco.mj_forward(self.model, self.data)
        
        obs = self._get_observation()
        up_z = float(self.data.xmat[self.base_bid, 8]) if self.base_bid >= 0 else 1.0
        info = {
            'height': self.data.qpos[2],
            'uprightness': up_z,
            'gravity': self.model.opt.gravity[2]
        }
        
        return obs, info
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Step environment"""
        self.current_step += 1
        action = np.clip(action, -1.0, 1.0).astype(np.float32)
        prev = self.prev_action.copy()
        self.prev_action = action.copy()
        
        reward = 0.0
        terminated = False
        info = {}
        
        # Simulation step
        for _ in range(self.decim):
            # L·∫•y joint positions v√† velocities
            q = []
            dq = []
            for joint_id in self.actuated_joint_ids:
                qpos_idx = self.model.jnt_qposadr[joint_id]
                qvel_idx = self.model.jnt_dofadr[joint_id]
                q.append(self.data.qpos[qpos_idx])
                dq.append(self.data.qvel[qvel_idx])
            
            q = np.array(q, dtype=np.float32)
            dq = np.array(dq, dtype=np.float32)
            
            # PD Control - THEO D·ª∞ √ÅN HO√ÄN TH√ÄNH
            target_q = self.default + action * self.action_scale
            tau = self._pd_control(q, dq, target_q)
            
            # ASSIST SYSTEM - H·ªñ TR·ª¢ ƒê·ªÇ ƒê·ª®NG V·ªÆNG
            if self.assist_enabled:
                # Assist cho vi·ªác gi·ªØ thƒÉng b·∫±ng
                assist_tau = self._compute_assist_torque(q, dq)
                tau += assist_tau * self.assist_strength
                # Gi·∫£m assist theo th·ªùi gian
                self.assist_strength *= self.assist_decay
            
            # GOVERNOR SYSTEM - KI·ªÇM SO√ÅT H√ÄNH ƒê·ªòNG
            if self.governor_enabled:
                tau = self._apply_governor(tau, q, dq)
            
            self.data.ctrl[:self.num_actions] = tau
            
            mujoco.mj_step(self.model, self.data)
            
            # Reward function - THEO D·ª∞ √ÅN HO√ÄN TH√ÄNH
            current_height = self.data.qpos[2]
            up_z = float(self.data.xmat[self.base_bid, 8]) if self.base_bid >= 0 else 1.0
            
            # Height reward - THEO CHI·ªÄU CAO TH·ª∞C T·∫æ C·ª¶A ROBOT (0.793m)
            target_height = 0.793
            height_reward = np.exp(-((current_height - target_height)**2) / (2 * 0.05**2))
            
            # Upright reward - M·∫†NH H·ªöN ƒë·ªÉ gi·ªØ thƒÉng b·∫±ng
            upright_reward = max(0.0, up_z) ** 2  # B√¨nh ph∆∞∆°ng ƒë·ªÉ tƒÉng c∆∞·ªùng
            
            # Stability reward - Th∆∞·ªüng cho vi·ªác gi·ªØ thƒÉng b·∫±ng
            stability_reward = 0.0
            if len(q) >= 12:
                # Penalty cho vi·ªác nghi√™ng qu√° m·ª©c
                base_roll = abs(q[3]) if len(q) > 3 else 0  # base roll
                base_pitch = abs(q[4]) if len(q) > 4 else 0  # base pitch
                stability_reward = -0.5 * (base_roll + base_pitch)
            
            # Energy penalty (nh·∫π)
            energy_penalty = -1e-4 * float((tau**2).mean())
            
            # Smooth penalty (nh·∫π)
            smooth_penalty = -1e-3 * float(((action - prev)**2).mean())
            
            # PENALTY M·∫†NH cho vi·ªác nh·∫£y l√™n cao
            if current_height > 1.0:  # N·∫øu cao h∆°n 1m (qu√° cao so v·ªõi 0.793m)
                height_penalty = -50.0 * (current_height - 1.0)  # Penalty m·∫°nh
            else:
                height_penalty = 0.0
            
            # PENALTY cho t∆∞ th·∫ø kh√¥ng t·ª± nhi√™n (tay dang r·ªông, ch√¢n nh·∫•c l√™n)
            # Penalty cho vi·ªác tay dang r·ªông qu√° m·ª©c
            arm_penalty = 0.0
            if len(q) >= 12:  # ƒê·∫£m b·∫£o c√≥ ƒë·ªß joint angles
                # Left arm (shoulder joints)
                left_shoulder_roll = abs(q[6]) if len(q) > 6 else 0  # left_shoulder_roll
                left_shoulder_pitch = abs(q[7]) if len(q) > 7 else 0  # left_shoulder_pitch
                # Right arm (shoulder joints)  
                right_shoulder_roll = abs(q[12]) if len(q) > 12 else 0  # right_shoulder_roll
                right_shoulder_pitch = abs(q[13]) if len(q) > 13 else 0  # right_shoulder_pitch
                
                # Penalty n·∫øu tay dang qu√° r·ªông
                arm_penalty = -0.1 * (left_shoulder_roll + left_shoulder_pitch + right_shoulder_roll + right_shoulder_pitch)
            
            reward += 0.6 * height_reward + 1.0 * upright_reward + stability_reward + energy_penalty + smooth_penalty + height_penalty + arm_penalty
            
            # Termination check - THEO D·ª∞ √ÅN HO√ÄN TH√ÄNH
            if self.terminate_on_fall and (current_height < 0.20 or up_z < 0.3):
                terminated = True
                break
        
        # Check timeout
        truncated = self.current_step >= self.max_episode_steps
        
        # Get observation
        obs = self._get_observation()
        
        # Info
        info = {
            "height": self.data.qpos[2],
            "uprightness": up_z if self.base_bid >= 0 else 1.0,
            "total_reward": reward
        }
        
        return obs, float(reward), terminated, truncated, info
    
    def render(self):
        """Render environment"""
        if self.render_mode == "human":
            try:
                if self.viewer is None:
                    import mujoco.viewer
                    self.viewer = mujoco.viewer.launch_passive(self.model, self.data)
                self.viewer.sync()
            except AttributeError:
                pass
    
    def close(self):
        """Close environment"""
        if self.viewer is not None:
            self.viewer.close()
            self.viewer = None


if __name__ == "__main__":
    """Test environment"""
    print("üß™ Testing G1StandingSimple...")
    
    env = G1StandingSimple()
    
    # Test m·ªôt episode
    obs, info = env.reset()
    print(f"‚úÖ Environment reset th√†nh c√¥ng!")
    print(f"   - Observation shape: {obs.shape}")
    print(f"   - Action shape: {env.action_space.shape}")
    
    total_reward = 0
    for step in range(100):
        action = env.action_space.sample() * 0.1  # Small actions
        obs, reward, done, truncated, info = env.step(action)
        total_reward += reward
        
        if step % 20 == 0:
            print(f"Step {step}: reward={reward:.3f}, height={info['height']:.3f}, "
                  f"uprightness={info['uprightness']:.3f}")
        
        if done or truncated:
            break
    
    print(f"‚úÖ Test completed!")
    print(f"   - Total reward: {total_reward:.3f}")
    print(f"   - Final height: {info['height']:.3f}")
    print(f"   - Final uprightness: {info['uprightness']:.3f}")
    
    env.close()
